{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4895863a-aba6-4cd0-92b2-16a92f9fe3a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style='text-align: center;'>\n",
    "    <h1><strong>GITHUB SCRAPER</strong></h1>\n",
    "    <h4 style='text-align: center;'> To create a <i>structured dataset</i> containing repository details by scraping top repositories for GitHub Topics<i>\n",
    "</div>\n",
    "        <p style='text-align: right;'><i>Github scraper by waqas ahmad</i></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd104ce-5cb7-4d0d-b717-6cfe18ae157f",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787ec75-287e-4060-8b07-9a18f626da70",
   "metadata": {},
   "source": [
    "The objective of this project is to scrape data from GitHub, a renowned platform for developers, and extract information about the top repositories within various GitHub topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbd189-8833-4674-baf7-4e0e8b16cd64",
   "metadata": {},
   "source": [
    "### Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f36f80-0fa9-4082-a2d7-e219dd7a6297",
   "metadata": {},
   "source": [
    "Our goal is to  scrap data about the top repositories within various **GitHub Topics section**  and create a structured dataset containing repository details, such as repository name, owner username, star count, and repository URL etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165136c4-7e52-4499-87d0-1fe01301f43b",
   "metadata": {
    "tags": []
   },
   "source": [
    "*This project serves as an exploration of web scraping techniques and data extraction from dynamic web pages*\n",
    "\n",
    "## Project Stack\n",
    "- **Programming Language:** Python\n",
    "- **Libraries:** requests, Beautiful Soup, Pandas\n",
    "- **Development Environment:** Jupyter Notebook\n",
    "\n",
    "## Project Outline\n",
    "### 1. Picking a Website and Defining the Objective\n",
    "- **Objective:** Select a website and define the project's objective.\n",
    "- **Steps:**\n",
    "   - I browsed through different sites and picked Github\n",
    "   - Lets identify the information to be scraped from the site.\n",
    "   - Decide the format of the output CSV file.\n",
    "\n",
    "### 2. Identifying Information to Scrape\n",
    "- **Objective:** Identify the key information to extract.\n",
    "- **Information to Scrape:**\n",
    "   - Topic Title\n",
    "   - Topic Page URL\n",
    "   - Topic Description\n",
    "   - Repository Name\n",
    "   - Owner Username\n",
    "   - Star Count\n",
    "   - Repository URL\n",
    "- **CSV Format:** Define the format for the output CSV file.\n",
    "\n",
    "### 3. Strategy and Project Summary\n",
    "- **Objective:** Outline the project strategy and summary.\n",
    "- **Steps:**\n",
    "   - Summarize the project idea and outline the strategy in a Jupyter notebook.\n",
    "   - Clarify objectives and plan the approach effectively.\n",
    "\n",
    "## Project Execution\n",
    "### 4. Step 1: Scraping GitHub List of Topics\n",
    "- **Objective:** Scrape the GitHub Topics page.\n",
    "- **Steps:**\n",
    "   - Begin data extraction by scraping the GitHub Topics page (https://github.com/topics).\n",
    "    - Use the requests library to download web pages.\n",
    "   - Utilize Beautiful Soup to parse and extract information.\n",
    "   - Convert data to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c307f66-c2d7-4f5d-809a-da58b48230ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_topics_page():\n",
    "    \"\"\"\n",
    "    Retrieves and parses the GitHub Topics page.\n",
    "\n",
    "    Returns:\n",
    "    BeautifulSoup object: Parsed HTML content of the GitHub Topics page.\n",
    "    \"\"\"\n",
    "    # URL of the GitHub Topics page\n",
    "    topics_url = 'https://github.com/topics'\n",
    "\n",
    "    # Send an HTTP GET request to the topics URL\n",
    "    response = requests.get(topics_url)\n",
    "\n",
    "    # Check if the response status code is 200 (OK)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Return the parsed document\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42444d51-41e1-4360-aa4f-bb99f8c484f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#invoking\n",
    "doc = get_topics_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b0f4b-87fd-4e65-9690-fb06ab88463d",
   "metadata": {},
   "source": [
    "Let's create some helper functions to parse information from the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d38e4a-9fd3-48b8-8366-465150167e5d",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Step 2: Collecting Topic Information\n",
    "- **Objective:** Collect information about GitHub topics.\n",
    "- **Information Collected:**\n",
    "   - Topic Title\n",
    "   - Topic Page URL\n",
    "   - Topic Description\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e0120-12aa-4829-9dc5-7ef2565fb68f",
   "metadata": {},
   "source": [
    "To get topic titles, we can pick `p` tags with the `class` ...\n",
    "\n",
    "![](1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3446e791-7408-4d11-96a9-ef5a7b69bd00",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lets define function to scrap p titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0283028-54c3-4a27-bcca-8d4029e0a111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of topic titles from a BeautifulSoup parsed document.\n",
    "\n",
    "    Args:\n",
    "    doc (BeautifulSoup object): Parsed HTML content of the GitHub Topics page.\n",
    "\n",
    "    Returns:\n",
    "    list of str: List of topic titles.\n",
    "    \"\"\"\n",
    "    # Define the CSS class for topic title elements\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "\n",
    "    # Find all elements with the specified class\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "\n",
    "    # Initialize an empty list to store topic titles\n",
    "    topic_titles = []\n",
    "\n",
    "    # Iterate through the found tags and extract text content\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "\n",
    "    # Return the list of topic titles\n",
    "    return topic_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cddbff6-923f-41e2-8cbe-87baf3053838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titles = get_topic_titles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4375ba-0129-4357-8c33-b0094952df1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171f56a2-a228-41f1-bfc9-f03f2aee7a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335598b3-a9ce-4936-aa64-b5e601a27887",
   "metadata": {},
   "source": [
    "Similarly we have defined functions for descriptions and URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b6cc6-2669-499d-8fa1-6d3964479164",
   "metadata": {},
   "source": [
    "Let's put this all together into a single function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d302d1-fe6d-4f21-91b9-d2733cf2607b",
   "metadata": {
    "tags": []
   },
   "source": [
    "To get topic description, we can pick `p` tags with the `class` as shown..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197cdf0-7bc2-4010-9491-48492a3a52f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](2.1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f8e182-106d-4d80-a45b-9522b60a4d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of topic descriptions from a BeautifulSoup parsed document.\n",
    "\n",
    "    Args:\n",
    "    doc (BeautifulSoup object): Parsed HTML content of the GitHub Topics page.\n",
    "\n",
    "    Returns:\n",
    "    list of str: List of topic descriptions.\n",
    "    \"\"\"\n",
    "    # Define the CSS class for topic description elements\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "\n",
    "    # Find all elements with the specified class\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "\n",
    "    # Initialize an empty list to store topic descriptions\n",
    "    topic_descs = []\n",
    "\n",
    "    # Iterate through the found tags, strip whitespace, and append to the list\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "\n",
    "    # Return the list of topic descriptions\n",
    "    return topic_descs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6289fa2-a869-4889-a1fd-a4185aaedb1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lets grab Topics URLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcac741-04a5-44e8-8fea-46a8180c409d",
   "metadata": {
    "tags": []
   },
   "source": [
    "To get topic description, we can pick `a` tags with the `class` as shown..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080472ec-c604-4bbb-bd0d-eea8845c337f",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48e8047f-b3e2-45d0-930e-546509a3e875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of topic page URLs from a BeautifulSoup parsed document.\n",
    "\n",
    "    Args:\n",
    "    doc (BeautifulSoup object): Parsed HTML content of the GitHub Topics page.\n",
    "\n",
    "    Returns:\n",
    "    list of str: List of topic page URLs.\n",
    "    \"\"\"\n",
    "    # Find all 'a' tags with the specified class\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "\n",
    "    # Initialize an empty list to store topic URLs\n",
    "    topic_urls = []\n",
    "\n",
    "    # Base URL for GitHub topics\n",
    "    base_url = 'https://github.com'\n",
    "\n",
    "    # Iterate through the found tags, extract 'href' attribute, and append to the list\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "\n",
    "    # Return the list of topic page URLs\n",
    "    return topic_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc6f2ba-7e2f-4199-86d2-37fde3bc30ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    \"\"\"\n",
    "    Scrapes and returns information about GitHub topics from the GitHub Topics page.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A Pandas DataFrame containing the following columns:\n",
    "        - 'title': List of topic titles.\n",
    "        - 'description': List of topic descriptions.\n",
    "        - 'url': List of topic page URLs.\n",
    "    \"\"\"\n",
    "    # URL of the GitHub Topics page\n",
    "    topics_url = 'https://github.com/topics'\n",
    "\n",
    "    # Send an HTTP GET request to the topics URL\n",
    "    response = requests.get(topics_url)\n",
    "\n",
    "    # Check if the response status code is 200 (OK)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topics_url))\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Create a dictionary with topic information\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a Pandas DataFrame\n",
    "    return pd.DataFrame(topics_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81c3a3b3-e861-4d88-bfb8-76efac811d24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>3D refers to the use of three-dimensional grap...</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>Algorithms are self-contained sequences that c...</td>\n",
       "      <td>https://github.com/topics/algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amp</td>\n",
       "      <td>Amp is a non-blocking concurrency library for ...</td>\n",
       "      <td>https://github.com/topics/amphp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Android</td>\n",
       "      <td>Android is an operating system built by Google...</td>\n",
       "      <td>https://github.com/topics/android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Angular</td>\n",
       "      <td>Angular is an open source web application plat...</td>\n",
       "      <td>https://github.com/topics/angular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ansible</td>\n",
       "      <td>Ansible is a simple and powerful automation en...</td>\n",
       "      <td>https://github.com/topics/ansible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>API</td>\n",
       "      <td>An API (Application Programming Interface) is ...</td>\n",
       "      <td>https://github.com/topics/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arduino</td>\n",
       "      <td>Arduino is an open source platform for buildin...</td>\n",
       "      <td>https://github.com/topics/arduino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASP.NET</td>\n",
       "      <td>ASP.NET is a web framework for building modern...</td>\n",
       "      <td>https://github.com/topics/aspnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Atom</td>\n",
       "      <td>Atom is a open source text editor built with w...</td>\n",
       "      <td>https://github.com/topics/atom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Awesome Lists</td>\n",
       "      <td>An awesome list is a list of awesome things cu...</td>\n",
       "      <td>https://github.com/topics/awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Amazon Web Services</td>\n",
       "      <td>Amazon Web Services provides on-demand cloud c...</td>\n",
       "      <td>https://github.com/topics/aws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Azure</td>\n",
       "      <td>Azure is a cloud computing service created by ...</td>\n",
       "      <td>https://github.com/topics/azure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Babel</td>\n",
       "      <td>Babel is a compiler for writing next generatio...</td>\n",
       "      <td>https://github.com/topics/babel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bash</td>\n",
       "      <td>Bash is a shell and command language interpret...</td>\n",
       "      <td>https://github.com/topics/bash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>Bitcoin is a cryptocurrency developed by Satos...</td>\n",
       "      <td>https://github.com/topics/bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bootstrap</td>\n",
       "      <td>Bootstrap is an HTML, CSS, and JavaScript fram...</td>\n",
       "      <td>https://github.com/topics/bootstrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bot</td>\n",
       "      <td>A bot is an application that runs automated ta...</td>\n",
       "      <td>https://github.com/topics/bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C</td>\n",
       "      <td>C is a general purpose programming language th...</td>\n",
       "      <td>https://github.com/topics/c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chrome</td>\n",
       "      <td>Chrome is a web browser from the tech company ...</td>\n",
       "      <td>https://github.com/topics/chrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chrome extension</td>\n",
       "      <td>Chrome extensions enable users to customize th...</td>\n",
       "      <td>https://github.com/topics/chrome-extension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Command line interface</td>\n",
       "      <td>A CLI, or command-line interface, is a console...</td>\n",
       "      <td>https://github.com/topics/cli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Clojure</td>\n",
       "      <td>Clojure is a dynamic, general-purpose programm...</td>\n",
       "      <td>https://github.com/topics/clojure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Code quality</td>\n",
       "      <td>Automate your code review with style, quality,...</td>\n",
       "      <td>https://github.com/topics/code-quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Code review</td>\n",
       "      <td>Ensure your code meets quality standards and s...</td>\n",
       "      <td>https://github.com/topics/code-review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Compiler</td>\n",
       "      <td>Compilers are software that translate higher-l...</td>\n",
       "      <td>https://github.com/topics/compiler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Continuous integration</td>\n",
       "      <td>Automatically build and test your code as you ...</td>\n",
       "      <td>https://github.com/topics/continuous-integration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>The coronavirus disease 2019 (COVID-19) is an ...</td>\n",
       "      <td>https://github.com/topics/covid-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>C++</td>\n",
       "      <td>C++ is a general purpose and object-oriented p...</td>\n",
       "      <td>https://github.com/topics/cpp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                        description  \\\n",
       "0                       3D  3D refers to the use of three-dimensional grap...   \n",
       "1                     Ajax  Ajax is a technique for creating interactive w...   \n",
       "2                Algorithm  Algorithms are self-contained sequences that c...   \n",
       "3                      Amp  Amp is a non-blocking concurrency library for ...   \n",
       "4                  Android  Android is an operating system built by Google...   \n",
       "5                  Angular  Angular is an open source web application plat...   \n",
       "6                  Ansible  Ansible is a simple and powerful automation en...   \n",
       "7                      API  An API (Application Programming Interface) is ...   \n",
       "8                  Arduino  Arduino is an open source platform for buildin...   \n",
       "9                  ASP.NET  ASP.NET is a web framework for building modern...   \n",
       "10                    Atom  Atom is a open source text editor built with w...   \n",
       "11           Awesome Lists  An awesome list is a list of awesome things cu...   \n",
       "12     Amazon Web Services  Amazon Web Services provides on-demand cloud c...   \n",
       "13                   Azure  Azure is a cloud computing service created by ...   \n",
       "14                   Babel  Babel is a compiler for writing next generatio...   \n",
       "15                    Bash  Bash is a shell and command language interpret...   \n",
       "16                 Bitcoin  Bitcoin is a cryptocurrency developed by Satos...   \n",
       "17               Bootstrap  Bootstrap is an HTML, CSS, and JavaScript fram...   \n",
       "18                     Bot  A bot is an application that runs automated ta...   \n",
       "19                       C  C is a general purpose programming language th...   \n",
       "20                  Chrome  Chrome is a web browser from the tech company ...   \n",
       "21        Chrome extension  Chrome extensions enable users to customize th...   \n",
       "22  Command line interface  A CLI, or command-line interface, is a console...   \n",
       "23                 Clojure  Clojure is a dynamic, general-purpose programm...   \n",
       "24            Code quality  Automate your code review with style, quality,...   \n",
       "25             Code review  Ensure your code meets quality standards and s...   \n",
       "26                Compiler  Compilers are software that translate higher-l...   \n",
       "27  Continuous integration  Automatically build and test your code as you ...   \n",
       "28                COVID-19  The coronavirus disease 2019 (COVID-19) is an ...   \n",
       "29                     C++  C++ is a general purpose and object-oriented p...   \n",
       "\n",
       "                                                 url  \n",
       "0                       https://github.com/topics/3d  \n",
       "1                     https://github.com/topics/ajax  \n",
       "2                https://github.com/topics/algorithm  \n",
       "3                    https://github.com/topics/amphp  \n",
       "4                  https://github.com/topics/android  \n",
       "5                  https://github.com/topics/angular  \n",
       "6                  https://github.com/topics/ansible  \n",
       "7                      https://github.com/topics/api  \n",
       "8                  https://github.com/topics/arduino  \n",
       "9                   https://github.com/topics/aspnet  \n",
       "10                    https://github.com/topics/atom  \n",
       "11                 https://github.com/topics/awesome  \n",
       "12                     https://github.com/topics/aws  \n",
       "13                   https://github.com/topics/azure  \n",
       "14                   https://github.com/topics/babel  \n",
       "15                    https://github.com/topics/bash  \n",
       "16                 https://github.com/topics/bitcoin  \n",
       "17               https://github.com/topics/bootstrap  \n",
       "18                     https://github.com/topics/bot  \n",
       "19                       https://github.com/topics/c  \n",
       "20                  https://github.com/topics/chrome  \n",
       "21        https://github.com/topics/chrome-extension  \n",
       "22                     https://github.com/topics/cli  \n",
       "23                 https://github.com/topics/clojure  \n",
       "24            https://github.com/topics/code-quality  \n",
       "25             https://github.com/topics/code-review  \n",
       "26                https://github.com/topics/compiler  \n",
       "27  https://github.com/topics/continuous-integration  \n",
       "28                https://github.com/topics/covid-19  \n",
       "29                     https://github.com/topics/cpp  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261d6f2-aaef-4c40-8a48-b0b47cc794cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lets Compare it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe953b0e-6787-4690-8ccd-7311a308dcd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f358c-d968-4fb3-a3a9-a9b8475223db",
   "metadata": {
    "tags": []
   },
   "source": [
    "Conclusion: **Verified**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec87f7ff-45ab-4827-9253-ecb806e83f09",
   "metadata": {},
   "source": [
    "Let's put this all together into a single function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21de57-e164-47dd-9761-eed1896bc6d7",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Step 3: Scraping Top Repositories per Topic\n",
    "- **Objective:** Scrape the top repositories within each GitHub topic.\n",
    "- **Steps:**\n",
    "   - Navigate to individual topic pages.\n",
    "   - Scrape data on the top 25 repositories in each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259f99b-39c3-42bb-9802-b0f4e658cb86",
   "metadata": {},
   "source": [
    "## Get the top 25 repositories from a topic page\n",
    "\n",
    "TODO - explanation and step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d66c-5776-4cde-9721-391b833c306f",
   "metadata": {},
   "source": [
    "Lets downloads and parse a GitHub topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e77205d3-ef50-414b-bd75-b725f8ffb1d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    \"\"\"\n",
    "    Downloads and parses a GitHub topic page.\n",
    "\n",
    "    Args:\n",
    "    topic_url (str): The URL of the GitHub topic page to download and parse.\n",
    "\n",
    "    Returns:\n",
    "    BeautifulSoup object: Parsed HTML content of the GitHub topic page.\n",
    "    \"\"\"\n",
    "    # Download the page using an HTTP GET request\n",
    "    response = requests.get(topic_url)\n",
    "\n",
    "    # Check if the response status code is 200 (OK)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Return the parsed document\n",
    "    return topic_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9466acb2-3c48-487a-ba87-67e62386687a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8de0b-aa48-4944-a28a-81311dcf0366",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### 7. Step 4: Repository Data Extraction\n",
    "- **Objective:** Extract data from repositories.\n",
    "- **Information Extracted:**\n",
    "   - Repository Name\n",
    "   - Owner Username\n",
    "   - Star Count\n",
    "   - Repository URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b8e63-ce84-4a0b-afa5-b400b712639f",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Lets Parse the star count of a GitHub repository*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d2b2ca-adfe-4935-b89e-924196155837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_star_count(stars):\n",
    "    \"\"\"\n",
    "    Parses the star count of a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "    stars (str): The star count as a string.\n",
    "\n",
    "    Returns:\n",
    "    int: The parsed star count as an integer.\n",
    "    \"\"\"\n",
    "    # Remove leading and trailing whitespace\n",
    "    stars = stars.strip()\n",
    "\n",
    "    # Check if the star count ends with 'k' (e.g., 5.5k)\n",
    "    if stars[-1] == 'k':\n",
    "        # Convert the string to a floating-point number and multiply by 1000\n",
    "        return int(float(stars[:-1])*1000)\n",
    "\n",
    "    # If not ending with 'k', parse as an integer\n",
    "    return int(stars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cacba6-6e5c-457a-a62e-be1b7b37f51d",
   "metadata": {},
   "source": [
    "*Lets extracts and return information about a GitHub repository*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99202f1-fdf3-4810-ac74-7e733718b486",
   "metadata": {},
   "source": [
    "Lets Grab `a` tag within `h3` tag with class as mentioned in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a536b-5c2c-4679-ac46-cfc07138d35b",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](5.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "538ff1db-c722-42c9-b772-ba0e6fc638b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag, star_tag):\n",
    "    \"\"\"\n",
    "    Extracts and returns information about a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "    h3_tag (BeautifulSoup object): The 'h3' tag containing repository information.\n",
    "    star_tag (BeautifulSoup object): The tag containing star count.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the following information:\n",
    "        - username (str): Owner's username.\n",
    "        - repo_name (str): Repository name.\n",
    "        - stars (int): Star count for the repository.\n",
    "        - repo_url (str): URL of the GitHub repository.\n",
    "    \"\"\"\n",
    "    # Find all 'a' tags within the 'h3' tag\n",
    "    a_tags = h3_tag.find_all('a')\n",
    "\n",
    "    # Extract username and repository name\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "\n",
    "    # Construct the repository URL using the base URL\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "\n",
    "    # Parse and extract the star count using the provided star_tag\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "\n",
    "    # Return the extracted information as a tuple\n",
    "    return username, repo_name, stars, repo_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed721d9-bac8-46d4-82dd-3d54831f3483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "613406c7-8e67-4375-9411-b9605e7c3693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \"\"\"\n",
    "    Extracts and returns information about top repositories within a GitHub topic.\n",
    "\n",
    "    Args:\n",
    "    topic_doc (BeautifulSoup object): Parsed HTML content of a GitHub topic page.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A Pandas DataFrame containing information about top repositories within the topic, with columns:\n",
    "        - 'username': Owner's username.\n",
    "        - 'repo_name': Repository name.\n",
    "        - 'stars': Star count for the repository.\n",
    "        - 'repo_url': URL of the GitHub repository.\n",
    "    \"\"\"\n",
    "    # Define the CSS class for repo title elements\n",
    "    h3_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "\n",
    "    # Find all 'h3' tags with the specified class\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': 'h3_selection_class'})\n",
    "\n",
    "    # Find star tags\n",
    "    star_tags = topic_doc.find_all('a', {'class': 'social-count float-none'})\n",
    "\n",
    "    # Initialize a dictionary to store repo information\n",
    "    topic_repos_dict = {\n",
    "        'username': [],\n",
    "        'repo_name': [],\n",
    "        'stars': [],\n",
    "        'repo_url': []\n",
    "    }\n",
    "\n",
    "    # Get repo info for each repository\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "\n",
    "    # Convert the dictionary to a Pandas DataFrame\n",
    "    return pd.DataFrame(topic_repos_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b1d7f-4f0d-4970-bfb9-23d42a8c19cc",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Step 5: Creating CSV Files\n",
    "- **Objective:** Create CSV files for each topic.\n",
    "- **CSV Format:** CSV files include columns for repository data in the specified format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad29d8-e0a0-4dc8-817c-5d42a6947b5e",
   "metadata": {},
   "source": [
    "*Lets scrapes and saves information about top repositories within a GitHub topic to a CSV file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f5bd2f6-e13a-4d61-85e5-54bbbb501bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "    \"\"\"\n",
    "    Scrapes and saves information about top repositories within a GitHub topic to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    topic_url (str): The URL of the GitHub topic page to scrape.\n",
    "    path (str): The path to the CSV file where the scraped data will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the CSV file already exists at the specified path\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "\n",
    "    # Get information about top repositories within the topic\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "\n",
    "    # Save the scraped data to a CSV file at the specified path\n",
    "    topic_df.to_csv(path, index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4547c4-3b06-4ffc-a451-63f76e9bc12f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9. Step 7: Putting it All Together\n",
    "- **Objective:** Create functions for each project step.\n",
    "- **Functions:** Functions were created to handle key steps efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f46316f8-26d9-4e2b-a975-42a3a8944855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    \"\"\"\n",
    "    Scrapes information about GitHub topics and their top repositories, saving the data in CSV files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Print a message to indicate the start of scraping\n",
    "    print('Scraping list of topics')\n",
    "\n",
    "    # Scrape information about GitHub topics and store it in a DataFrame\n",
    "    topics_df = scrape_topics()\n",
    "\n",
    "    # Create a directory named 'data' if it doesn't exist\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "\n",
    "    # Iterate through each topic and scrape top repositories\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']))\n",
    "        \n",
    "        # Specify the path for the CSV file using the topic title\n",
    "       # Scrape and save top repositories for the current topic\n",
    "       \n",
    "\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6713f-b41f-4639-99b2-872cbfcf7c58",
   "metadata": {},
   "source": [
    "Let's run it to scrape the top repos for the all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f00e159f-3554-4340-8f9d-8c2ba0636174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories for \"3D\"\n",
      "Scraping top repositories for \"Ajax\"\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "Scraping top repositories for \"Amp\"\n",
      "Scraping top repositories for \"Android\"\n",
      "Scraping top repositories for \"Angular\"\n",
      "Scraping top repositories for \"Ansible\"\n",
      "Scraping top repositories for \"API\"\n",
      "Scraping top repositories for \"Arduino\"\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "Scraping top repositories for \"Atom\"\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "Scraping top repositories for \"Azure\"\n",
      "Scraping top repositories for \"Babel\"\n",
      "Scraping top repositories for \"Bash\"\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "Scraping top repositories for \"Bootstrap\"\n",
      "Scraping top repositories for \"Bot\"\n",
      "Scraping top repositories for \"C\"\n",
      "Scraping top repositories for \"Chrome\"\n",
      "Scraping top repositories for \"Chrome extension\"\n",
      "Scraping top repositories for \"Command line interface\"\n",
      "Scraping top repositories for \"Clojure\"\n",
      "Scraping top repositories for \"Code quality\"\n",
      "Scraping top repositories for \"Code review\"\n",
      "Scraping top repositories for \"Compiler\"\n",
      "Scraping top repositories for \"Continuous integration\"\n",
      "Scraping top repositories for \"COVID-19\"\n",
      "Scraping top repositories for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11eeced-7157-4d4d-be52-1c40fd15ee9b",
   "metadata": {},
   "source": [
    "We can check that the CSVs were created properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d0186-a3e3-4a4d-af85-23ac23521a93",
   "metadata": {},
   "source": [
    "# Output Files\n",
    "\n",
    "Output Files can be seen showing the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510035b-d813-4d32-8b18-36d5c0285ca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "![](Results.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5194f-085c-422c-bd75-fb728442a5e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Top Repos for 3D**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd92f7c8-3c9e-4748-ba27-d7fb5a2bb0f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](excel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ef894-f344-4fed-aca4-af6bb00197d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "- **Successful Data Extraction:** This project demonstrated our proficiency in web scraping techniques, enabling us to successfully extract valuable data from dynamic websites.\n",
    "- **Structured Datasets:** We organized the extracted information into structured datasets, providing valuable resources for developers seeking repositories in various programming domains.\n",
    "- **Insights into User Engagement:** By focusing on GitHub topics and repositories, we gained insights into user engagement dynamics on the platform.\n",
    "- **Portfolio Enhancement:** \"Scraping Top Repositories for GitHub Topics\" adds to my portfolio of data-related skills and showcases my ability to tackle real-world data extraction projects.\n",
    "- **Contribution to Data Analysis:** The project contributes to the broader field of data analysis and web scraping, highlighting the practical applications of these skills.\n",
    "- **Detailed Code and Implementation:** we witnessed this Jupyter Notebook, which contains the complete code and implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a188593-9000-4c73-a8ef-33f0a9752d52",
   "metadata": {},
   "source": [
    "***Thanks for being my life's audience, even when I'm just a comedy of errors!***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
